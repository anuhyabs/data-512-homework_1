{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA ACQUISITION\n",
    "\n",
    "In this notebook, the data on counts of pageviews from desktop, mobile web and mobile app is collected for a subset of English Wikipedia dinosaur related articles.\n",
    "\n",
    "The output of the notebook are three datasets saved as JSON files ordered using article titles as a key for the resulting time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json, time, urllib.parse\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have defined the values of constants such as the API pageviews URL, headers, file from which the titles of the dinosaur articles are imported, name of the files to where the JSON needs to be exported in the following block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "# The REST API 'pageviews' URL - this is the common URL/endpoint for all 'pageviews' API requests\n",
    "API_REQUEST_PAGEVIEWS_ENDPOINT = 'https://wikimedia.org/api/rest_v1/metrics/pageviews/'\n",
    "\n",
    "# This is a parameterized string that specifies what kind of pageviews request we are going to make\n",
    "# In this case it will be a 'per-article' based request. The string is a format string so that we can\n",
    "# replace each parameter with an appropriate value before making the request\n",
    "API_REQUEST_PER_ARTICLE_PARAMS = 'per-article/{project}/{access}/{agent}/{article}/{granularity}/{start}/{end}'\n",
    "\n",
    "# The Pageviews API asks that we not exceed 100 requests per second, we add a small delay to each request\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "\n",
    "# When making a request to the Wikimedia API they ask that you include a \"unique ID\" that will allow them to\n",
    "# contact you if something happens - such as - your code exceeding request limits - or some other error happens\n",
    "REQUEST_HEADERS = {\n",
    "    'User-Agent': '<anuhyabs@uw.edu>, University of Washington, MSDS DATA 512 - AUTUMN 2022',\n",
    "}\n",
    "\n",
    "# This is a subset of the English Wikipedia dinosaur related files.\n",
    "ARTICLE_TITLES = pd.read_csv('../data/dinosaur_genera.cleaned.SEPT.2022.csv')[\"name\"]\n",
    "START_DATE = \"2015070100\"\n",
    "END_DATE = \"2022093000\"\n",
    "\n",
    "DESKTOP_DATA = \"../data/dino_monthly_desktop_201501-202209.json\"\n",
    "MOBILE_DATA = \"../data/dino_monthly_mobile_201501-202209.json\"\n",
    "CUMULATIVE_DATA = \"../data/dino_monthly_cumulative_201501-202209.json\" \n",
    "\n",
    "# This template is used to map parameter values into the API_REQUST_PER_ARTICLE_PARAMS portion of an API request. The dictionary has a\n",
    "# field/key for each of the required parameters. \n",
    "ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE = {\n",
    "    \"project\":     \"en.wikipedia.org\",\n",
    "    \"access\":      \"\",      # this value will be changed for the different access types\n",
    "    \"agent\":       \"user\",\n",
    "    \"article\":     \"\",             # this value will be set/changed before each request\n",
    "    \"granularity\": \"monthly\",\n",
    "    \"start\":       START_DATE,\n",
    "    \"end\":         END_DATE   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The request_pageviews_per_article is a function that takes in the article titles, access type, endpoint url, parameters (defined earlier) and headers makes the request for the information related to the article. This function returns a JSON (dictionary type) with all the information on the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    PROCEDURES/FUNCTIONS\n",
    "#\n",
    "\n",
    "def request_pageviews_per_article(article_title = None, \n",
    "                                  access = \"desktop\",\n",
    "                                  endpoint_url = API_REQUEST_PAGEVIEWS_ENDPOINT, \n",
    "                                  endpoint_params = API_REQUEST_PER_ARTICLE_PARAMS, \n",
    "                                  request_template = ARTICLE_PAGEVIEWS_PARAMS_TEMPLATE,\n",
    "                                  headers = REQUEST_HEADERS):\n",
    "    # Make sure we have an article title\n",
    "    if not article_title: return None\n",
    "    \n",
    "    # Titles are supposed to have spaces replaced with \"_\" and be URL encoded\n",
    "    article_title_encoded = urllib.parse.quote(article_title.replace(' ','_'))\n",
    "    request_template['article'] = article_title_encoded\n",
    "    request_template['access'] = access\n",
    "\n",
    "    \n",
    "    # now, create a request URL by combining the endpoint_url with the parameters for the request\n",
    "    request_url = endpoint_url+endpoint_params.format(**request_template)\n",
    "    \n",
    "    # make the request\n",
    "    try:\n",
    "        # we'll wait first, to make sure we don't exceed the limit in the situation where an exception\n",
    "        # occurs during the request processing - throttling is always a good practice with a free\n",
    "        # data source like Wikipedia - or other community sources\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generate_views function takes in the subset of articles and accesstype as parameters. It loops through each article in the articles subset and calls the request_pageviews_per_article function for information on the article. \n",
    "\n",
    "For mobile access type, there are two seperate requests so the code combines the monthly pageviews for both mobile-app and mobile-web and returns the combined mobile views.\n",
    "\n",
    "For cummulative access type, I generate the cummulative pageviews for all the months until September 2022 (by adding the monthly views of the priors months to each month) and return the the total pageviews for all access types.\n",
    "\n",
    "For desktop access type, the montly pageviews is returned as is from therequest API.\n",
    "\n",
    "This function returns a dictionary containing each article title as the key and it's relevant information as the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to generate datasets for all articles at different access levels\n",
    "def generate_views(articles, access):\n",
    "    views_json = {}\n",
    "\n",
    "    if access == \"mobile\":\n",
    "        # The API separates mobile access types into two separate requests: mobile-app and mobile-web.\n",
    "        # The following code sums these to make one count for all mobile pageviews.\n",
    "        mobile_views = {}\n",
    "        for title in articles:\n",
    "            mobile_app_views = request_pageviews_per_article(title, \"mobile-app\")\n",
    "            mobile_web_views = request_pageviews_per_article(title, \"mobile-web\")\n",
    "        \n",
    "            mobile_app_views = mobile_app_views['items']\n",
    "            mobile_web_views = mobile_web_views['items']\n",
    "            mobile_views = mobile_app_views\n",
    "            for month in range(len(mobile_web_views)):\n",
    "                mon = mobile_web_views[month]\n",
    "                views = mon['views']\n",
    "                mobile_views[month]['views'] += views\n",
    "            views_json[title] = mobile_views\n",
    "    elif access == \"all-access\":\n",
    "        for title in articles:\n",
    "            views = request_pageviews_per_article(title, access)\n",
    "            total_views = 0\n",
    "            views = views['items']\n",
    "            for month in views:\n",
    "                total_views += month['views']\n",
    "                month['views'] = total_views\n",
    "            views_json[title] = views\n",
    "    else:        \n",
    "        for title in articles:\n",
    "            views = request_pageviews_per_article(title, access)\n",
    "            views = views['items']\n",
    "            views_json[title] = views\n",
    "\n",
    "    return views_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The write_file function saves the the dictionary received from the previous function into JSON files. As previously stated, the file name and path to save the JSON file to has already been defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the datasets as JSON files\n",
    "def write_file(path, views):\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(views, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I call the function for each of the three access types in the following steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating monthly desktop views\n",
    "desktop_views = generate_views(ARTICLE_TITLES, \"desktop\")\n",
    "write_file(DESKTOP_DATA, desktop_views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating monthly cumulative views\n",
    "cumulative_views = generate_views(ARTICLE_TITLES, \"all-access\")\n",
    "write_file(CUMULATIVE_DATA, cumulative_views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating monthly mobile views\n",
    "mobile_views = generate_views(ARTICLE_TITLES, \"mobile\")\n",
    "write_file(MOBILE_DATA, mobile_views)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7019494c316389886f5f5fcb2e396e2fee403f1eb1d04a04c662a640ad80dd4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
